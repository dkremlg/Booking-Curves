{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data=pd.read_csv('R_Input.csv')\n",
    "Data['dprio0']=Data['dprio']**0\n",
    "Data['dprio2']=Data['dprio']**2\n",
    "Data['dprio3']=Data['dprio']**3\n",
    "Data=Data.rename(columns={'dprio': 'dprio1'})\n",
    "\n",
    "direction=pd.get_dummies(Data['direction'],drop_first=True)\n",
    "direction.columns=['direction:'+x for x in direction.columns]\n",
    "\n",
    "whichholiday_left_LUX=pd.get_dummies(Data['whichholiday_left_LUX'],drop_first=True)\n",
    "whichholiday_left_LUX.columns=['whichholiday_left_LUX:'+x for x in whichholiday_left_LUX.columns]\n",
    "\n",
    "whichholiday_right_LUX=pd.get_dummies(Data['whichholiday_right_LUX'],drop_first=True)\n",
    "whichholiday_right_LUX.columns=['whichholiday_right_LUX:'+x for x in whichholiday_right_LUX.columns]\n",
    "\n",
    "Data=pd.concat([Data,direction,whichholiday_left_LUX,whichholiday_right_LUX],axis=1)\n",
    "\n",
    "Data.index=Data['calendar_date']+'_'+Data['direction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "dprio=Data[['dprio'+str(i) for i in range(4)]].drop_duplicates().transpose()\n",
    "dprio.columns=range(365,-1,-1)\n",
    "\n",
    "X_data=Data.drop_duplicates(subset=['calendar_date','direction'])[[x for x in Data.columns if 'direction:' in x]\\\n",
    "+[x for x in Data.columns if 'whichholiday_left_LUX:' in x]\\\n",
    "+[x for x in Data.columns if 'whichholiday_right_LUX:' in x]\\\n",
    "+['holiday_from_left_LUX','holiday_from_right_LUX','holiday_to_left_LUX','holiday_to_right_LUX']\\\n",
    "+['pax_total']]\n",
    "\n",
    "Data['concat']=Data['calendar_date']+'_'+Data['direction']\n",
    "y_data=Data[['concat','dprio1','pax_total']].pivot(index='concat',columns='dprio1',values='pax_total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train=X.loc[[x for x in X.index if '2019' not in x],:].values\n",
    "# X_test=X.loc[[x for x in X.index if '2019' in x],:].values\n",
    "# y_train=y.loc[[x for x in y.index if '2019' not in x],:].values\n",
    "# y_test=y.loc[[x for x in y.index if '2019' in x],:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input=X_train.shape[1]\n",
    "n_hidden_1=n_input*2\n",
    "n_hidden_2=4\n",
    "n_output=y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dprio=tf.convert_to_tensor(dprio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 100\n",
    "\n",
    "\n",
    "Z=tf.constant(dprio.values, dtype=tf.float32)\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_output])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Dictionary of Weights and Biases\n",
    "weights = {\n",
    "  'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "  'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "  'out': tf.Variable(tf.random_normal([n_hidden_2, n_output]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "  'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "  'b2': tf.Variable(tf.random_normal([n_hidden_2]))\n",
    "}\n",
    "\n",
    "# Model ForwarÂ§d Propagation step\n",
    "def forward_propagation(x):\n",
    "    # Hidden layer1\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    \n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "\n",
    "    # Output fully connected layer\n",
    "    out_layer = tf.matmul(layer_2, Z) \n",
    "    return out_layer\n",
    "\n",
    "\n",
    "# Model Outputs\n",
    "yhat = forward_propagation(X)\n",
    "ypredict = tf.argmax(yhat, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=yhat))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_op = optimizer.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataFrame' objects are mutable, thus they cannot be hashed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-304-e2cfcf77dc1a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;31m#Stochasting Gradient Descent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m             \u001b[0msummary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mtrain_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mypredict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__hash__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1490\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1491\u001b[0m         raise TypeError('{0!r} objects are mutable, thus they cannot be'\n\u001b[1;32m-> 1492\u001b[1;33m                         ' hashed'.format(self.__class__.__name__))\n\u001b[0m\u001b[0;32m   1493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1494\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'DataFrame' objects are mutable, thus they cannot be hashed"
     ]
    }
   ],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "from datetime import datetime\n",
    "startTime = datetime.now()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    #writer.add_graph(sess.graph)\n",
    "    #EPOCHS\n",
    "    for epoch in range(training_epochs):\n",
    "        #Stochasting Gradient Descent\n",
    "        for i in range(len(X_train)):\n",
    "            summary = sess.run(train_op, feed_dict={X: X_train.loc[i: i + 1,:], y: y_train.loc[i:i + 1,:]})\n",
    "        \n",
    "        train_accuracy = np.mean(np.argmax(y_train, axis=1) == sess.run(ypredict, feed_dict={X: X_train, y: y_train}))\n",
    "        test_accuracy  = np.mean(np.argmax(y_test, axis=1) == sess.run(ypredict, feed_dict={X: X_test, y: y_test}))\n",
    "                \n",
    "        print(\"Epoch = %d, train accuracy = %.2f%%, test accuracy = %.2f%%\" % (epoch + 1, 100. * train_accuracy, 100. * test_accuracy))\n",
    "        #print(\"Epoch = %d, train accuracy = %.2f%%\" % (epoch + 1, 100. * train_accuracy))\n",
    "\n",
    "    sess.close()\n",
    "print(\"Time taken:\", datetime.now() - startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['concat']=Data['calendar_date']+'_'+Data['direction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                                                         \n",
    "##########################################\n",
    "\n",
    "X_cluster=Data.drop_duplicates(subset=['calendar_date','direction'])[['direction','whichholiday_left_LUX'\n",
    ",'whichholiday_right_LUX','holiday_from_left_LUX','holiday_from_right_LUX','holiday_to_left_LUX','holiday_to_right_LUX']]\n",
    "#.as_matrix()\n",
    "X_cluster.index=y.index\n",
    "\n",
    "##########################################\n",
    "\n",
    "X_prio=Data.drop_duplicates(subset=['calendar_date','direction'])[['direction','whichholiday_left_LUX'\n",
    ",'whichholiday_right_LUX','holiday_from_left_LUX','holiday_from_right_LUX','holiday_to_left_LUX','holiday_to_right_LUX']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "############################\n",
    "\n",
    "X_dprio=Data['dprio'].unique()\n",
    "X_dprio2=[x**2 for x in Data['dprio'].unique()]\n",
    "X_dprio3=[x**3 for x in Data['dprio'].unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_cluster,y, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split(Data, mnist.target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
